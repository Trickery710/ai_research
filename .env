# =============================================================
# AI Research Refinery v2 - GPU Configuration
# =============================================================
# device_ids in docker-compose.yml lock each Ollama container
# to exactly one GPU. No model splitting across GPUs.
#
# Check your GPU IDs with: nvidia-smi -L
#   GPU 0: Quadro P1000 (4GB)  -> embeddings (nomic-embed-text)
#   GPU 1: RTX 3080 (10GB)     -> reasoning (mistral) + reason2
#   GPU 2: RTX 3070 (8GB)      -> unused / available
#
# Single GPU: set both to 0
# GPU_EMBED=0
# GPU_REASON=0

GPU_EMBED=0
GPU_REASON=1
