# =============================================================
# AI Research Refinery v2 - GPU Configuration
# =============================================================
# Controls which GPU(s) each Ollama instance can see.
#
# Values:
#   all   = share all available GPUs (default, works everywhere)
#   0     = use only GPU 0
#   1     = use only GPU 1
#   0,1   = use GPUs 0 and 1
#   none  = CPU-only mode (no GPU acceleration)
#
# Single GPU system (default):
#   Both instances share the same GPU. Ollama manages VRAM
#   automatically by loading/unloading models as needed.
GPU_EMBED=all
GPU_REASON=all

# Dual GPU system (P340 production config - uncomment these):
#   GPU 0 (RTX 3080, 10GB) -> embeddings (nomic-embed-text ~0.5GB)
#   GPU 1 (RTX 3070, 8GB)  -> reasoning  (llama3 8B q4 ~4.5GB)
# GPU_EMBED=0
# GPU_REASON=1
