AI Research Refinery v3

Self-Hosted Autonomous Automotive Knowledge Engine

1. System Requirements

Hardware (P340 Configuration)
  64GB RAM
  NVIDIA 3080 (GPU 0 - Embeddings)
  NVIDIA 3070 (GPU 1 - Reasoning)
  NVIDIA P1000 (optional, display)
  NVMe storage recommended

Software Required
  docker --version
  docker compose version
  nvidia-smi

If Docker is missing:
  sudo apt install docker.io docker-compose-plugin

2. Enable NVIDIA GPU for Docker

Install NVIDIA Container Toolkit:
  distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
  curl -s -L https://nvidia.github.io/libnvidia-container/gpgkey | sudo apt-key add -
  curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
  sudo apt update
  sudo apt install -y nvidia-container-toolkit
  sudo systemctl restart docker

Verify:
  docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi

3. Getting Started

  cd ai_research
  docker compose up --build

First startup pulls Ollama models (nomic-embed-text, llama3) automatically.
This may take several minutes.

4. Architecture Overview (18 Containers)

Infrastructure (5 services):
  Service              Purpose                        Port
  PostgreSQL+pgvector  Database with vector search     localhost:5432
  Redis                Job queues & rate limiting      localhost:6379
  MinIO                Object/document storage         localhost:9000 (API), 9001 (console)
  Ollama (embed)       Embedding model server          localhost:11434
  Ollama (reason)      Reasoning model server          localhost:11435

Application Layer (1 service):
  Backend (FastAPI)    REST API                        localhost:8000

Pipeline Workers (6 services):
  worker-crawler       Fetches URLs, extracts text     jobs:crawl -> jobs:chunk
  worker-chunking      Splits documents into chunks    jobs:chunk -> jobs:embed
  worker-embedding     Generates vector embeddings     jobs:embed -> jobs:evaluate
  worker-evaluation    LLM scores trust/relevance      jobs:evaluate -> jobs:extract
  worker-extraction    Extracts DTCs, causes, steps    jobs:extract -> jobs:resolve
  worker-conflict      Resolves duplicate entities     jobs:resolve -> (terminal)

Monitoring & Self-Healing (2 services):
  monitor-agent        Metrics collection, alerting    localhost:8001
  healing-agent        Auto-fixes pipeline issues      (internal)

Autonomous Orchestration (4 services):
  orchestrator         OODA loop, task management      (internal)
  auditor              Quality/coverage analysis       (internal)
  researcher           URL discovery & submission      (internal)
  mcp-server           MCP knowledge API (SSE)         localhost:8002

5. Processing Pipeline

Documents flow through a 6-stage pipeline:

  POST /crawl or POST /ingest
      |
  Crawl -> Chunk -> Embed -> Evaluate -> Extract -> Resolve
      |         |        |          |          |         |
  Fetch URL  500-char  768-dim   Trust &   DTCs,     Merge
  or store   chunks    vectors   relevance causes,   duplicates
  in MinIO   w/overlap           scoring   steps

Each stage passes document IDs through Redis queues (FIFO via LPUSH/BRPOP).

6. Autonomous Operation

The system runs autonomously once started:

  Orchestrator (60s cycle):
    1. Observe - collect queue depths, task statuses, audit data
    2. Orient  - assess resource availability, coverage gaps
    3. Decide  - rule-based for routine, LLM for strategic planning
    4. Act     - dispatch research tasks, trigger audits, or wait

  Auditor (30-min cycle):
    - Confidence score distributions across all DTC codes
    - Completeness scoring per DTC (description, causes, steps, sensors, TSBs)
    - DTC range gap detection (e.g., P02xx has 5 codes, expected ~30)
    - Pipeline throughput, error rates, bottleneck detection
    - Stores reports in research.audit_reports

  Researcher (directive-driven):
    - Tier 1: Deterministic URL templates (obd-codes.com, engine-codes.com, etc.)
    - Tier 2: LLM-suggested URLs from known automotive resources
    - Rate limited: 30 URLs/hr total, 5/domain/hr, 30s cooldown
    - Validates URLs via HEAD request before submitting to crawl pipeline

  MCP Server (port 8002):
    - lookup_dtc: Full DTC detail with causes, steps, sensors, TSBs
    - search_knowledge: Semantic vector search across knowledge base
    - list_dtc_codes: List/filter codes by category and confidence
    - get_system_stats: Coverage and quality metrics

Inter-Agent Communication (Redis queues):
  orchestrator:research   orchestrator -> researcher
  orchestrator:audit      orchestrator -> auditor
  orchestrator:commands   auditor/researcher/API -> orchestrator

Resource Protection:
  - GPU available = evaluate + extract + embed queues < 20
  - Crawl slots available = crawl + chunk queues < 5
  - If pipeline busy, orchestrator defers new work
  - Existing pipeline workers always have priority

Environment Toggles:
  ORCHESTRATOR_AUTO_RESEARCH=true/false  Enable/disable autonomous research
  ORCHESTRATOR_CYCLE=60                  Seconds between orchestrator cycles
  AUDIT_INTERVAL=1800                    Seconds between automatic audits
  MAX_URLS_PER_HOUR=30                   Global URL submission rate limit
  MAX_PER_DOMAIN_PER_HOUR=5             Per-domain rate limit

7. API Reference

Health & Status:
  GET  /health                  System health (DB, Redis, MinIO connectivity)
  GET  /stats                   Document counts, queue depths, DTC stats

Document Management:
  POST /ingest                  Ingest raw text content
  POST /crawl                   Submit URL for crawling
  GET  /crawl                   List crawl queue entries
  GET  /documents               List documents (optional stage filter)
  GET  /documents/{id}          Document detail
  GET  /documents/{id}/chunks   Document chunks with evaluation scores
  GET  /documents/{id}/status   Processing status with full log

Search & Knowledge:
  POST /search                  Semantic vector search
  GET  /dtc                     List DTC codes (filter by category/confidence)
  GET  /dtc/{code}              Full DTC detail (causes, steps, sensors, TSBs)

Orchestration:
  GET  /orchestrator/status     Current orchestrator state and cycle info
  GET  /orchestrator/tasks      List orchestrator tasks (filter by status/type)
  POST /orchestrator/command    Submit manual command (trigger_audit, trigger_research)
  GET  /audit/latest            Latest audit report
  GET  /audit/reports           List audit reports
  GET  /coverage                Latest coverage snapshot
  GET  /research/plans          List research plans
  GET  /research/sources        List known source domains with quality tiers

8. Database Schema

research schema (Raw/Processing Layer):
  documents              Document metadata and processing state
  document_chunks        Chunked text with 768-dim vector embeddings
  chunk_evaluations      LLM trust/relevance scores per chunk
  crawl_queue            URL crawl job tracking
  processing_log         Audit log for all pipeline stages
  healing_log            Self-healing action audit trail
  orchestrator_tasks     Task management for orchestrator
  orchestrator_log       Orchestrator cycle audit trail
  research_sources       Domain quality and crawl tracking
  research_plans         Research plan tracking
  audit_reports          Quality audit reports
  coverage_snapshots     Daily DTC coverage snapshots

refined schema (Structured Knowledge Layer):
  dtc_codes              Diagnostic trouble codes with confidence scores
  dtc_sources            Links DTC codes to source chunks
  diagnostic_steps       Step-by-step diagnostic procedures
  causes                 Root causes per DTC code
  sensors                Automotive sensor catalog
  tsb_references         Technical Service Bulletins

9. GPU Configuration

Controlled via .env file:
  Single GPU (shared):   GPU_EMBED=all  GPU_REASON=all
  Dual GPU (isolated):   GPU_EMBED=0    GPU_REASON=1

Monitor GPU usage:
  watch -n1 nvidia-smi

10. Common Operations

Submit a URL for autonomous processing:
  curl -X POST http://localhost:8000/crawl \
    -H "Content-Type: application/json" \
    -d '{"url": "https://www.obd-codes.com/p0301"}'

Trigger a manual audit:
  curl -X POST http://localhost:8000/orchestrator/command \
    -H "Content-Type: application/json" \
    -d '{"action": "trigger_audit"}'

Trigger research for specific codes:
  curl -X POST http://localhost:8000/orchestrator/command \
    -H "Content-Type: application/json" \
    -d '{"action": "trigger_research", "target_codes": ["P0301", "P0302"]}'

Check system status:
  curl http://localhost:8000/stats
  curl http://localhost:8000/orchestrator/status
  curl http://localhost:8000/audit/latest
  curl http://localhost:8000/coverage

Search the knowledge base:
  curl -X POST http://localhost:8000/search \
    -H "Content-Type: application/json" \
    -d '{"query": "P0301 misfire causes", "limit": 5}'

Look up a DTC code:
  curl http://localhost:8000/dtc/P0301

11. MCP Server (Model Context Protocol)

The MCP server exposes the knowledge base to AI tools (Claude Desktop,
Claude Code, Cursor, custom agents) via the Model Context Protocol.

Server details:
  Transport:  SSE (Server-Sent Events)
  SSE URL:    http://localhost:8002/sse
  Messages:   http://localhost:8002/messages/
  Health:     http://localhost:8002/health

Available tools:
  lookup_dtc         Look up a DTC code with causes, steps, sensors, TSBs
  search_knowledge   Semantic vector search across the knowledge base
  list_dtc_codes     List/filter DTC codes by category and confidence
  get_system_stats   Knowledge base coverage and quality metrics

--- Claude Desktop ---

Edit ~/Library/Application Support/Claude/claude_desktop_config.json (macOS)
  or %APPDATA%\Claude\claude_desktop_config.json (Windows):

  {
    "mcpServers": {
      "automotive-kb": {
        "transport": {
          "type": "sse",
          "url": "http://YOUR_SERVER_IP:8002/sse"
        }
      }
    }
  }

Replace YOUR_SERVER_IP with your P340's IP address (or localhost if running
on the same machine). Restart Claude Desktop after saving.

--- Claude Code (CLI) ---

Add to your project's .mcp.json file:

  {
    "mcpServers": {
      "automotive-kb": {
        "type": "sse",
        "url": "http://YOUR_SERVER_IP:8002/sse"
      }
    }
  }

Or add globally via:
  claude mcp add automotive-kb --transport sse http://YOUR_SERVER_IP:8002/sse

--- Cursor ---

Add to .cursor/mcp.json in your project root:

  {
    "mcpServers": {
      "automotive-kb": {
        "transport": "sse",
        "url": "http://YOUR_SERVER_IP:8002/sse"
      }
    }
  }

--- Python client (programmatic access) ---

  pip install mcp

  import asyncio
  from mcp.client.session import ClientSession
  from mcp.client.sse import sse_client

  async def main():
      async with sse_client("http://localhost:8002/sse") as (read, write):
          async with ClientSession(read, write) as session:
              await session.initialize()

              # List available tools
              tools = await session.list_tools()
              print([t.name for t in tools.tools])

              # Look up a DTC code
              result = await session.call_tool("lookup_dtc", {"code": "P0301"})
              print(result.content[0].text)

              # Semantic search
              result = await session.call_tool("search_knowledge", {
                  "query": "engine misfire causes",
                  "limit": 5,
                  "min_trust": 0.3
              })
              print(result.content[0].text)

              # List all powertrain codes
              result = await session.call_tool("list_dtc_codes", {
                  "category": "Powertrain",
                  "min_confidence": 0.5,
                  "limit": 20
              })
              print(result.content[0].text)

              # Get system stats
              result = await session.call_tool("get_system_stats", {})
              print(result.content[0].text)

  asyncio.run(main())

--- Testing the connection ---

Verify the server is running:
  curl http://localhost:8002/health

Test SSE stream (should see an endpoint event):
  curl -N http://localhost:8002/sse

--- Network access ---

The MCP server listens on port 8002. To access from other machines on
your network, ensure port 8002 is open and use the server's LAN IP.

Find your server IP:
  hostname -I | awk '{print $1}'

For remote access over the internet, set up a reverse proxy (nginx/caddy)
with TLS, or use a tunnel (cloudflared, tailscale).

12. Backups

Backup Postgres:
  docker exec refinery_postgres pg_dump -U refinery refinery > backup.sql

Restore:
  cat backup.sql | docker exec -i refinery_postgres psql -U refinery refinery

MinIO data is in the minio_data Docker volume.

13. Troubleshooting

View logs for any service:
  docker compose logs -f orchestrator
  docker compose logs -f auditor
  docker compose logs -f researcher
  docker compose logs -f mcp-server
  docker compose logs -f worker-crawler

Check container status:
  docker compose ps

Restart a specific service:
  docker compose restart orchestrator

GPU not detected in Ollama:
  docker exec -it refinery_llm_embed nvidia-smi
  If empty -> NVIDIA container toolkit not configured (see section 2)

Database connection issues:
  docker compose logs postgres
  Ensure postgres container is healthy: docker compose ps

Reset database (destructive):
  docker compose down -v
  docker compose up --build
