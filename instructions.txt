AI Research Refinery v3

Self-Hosted Autonomous Automotive Knowledge Engine

1. System Requirements

Hardware (P340 Configuration)
  64GB RAM
  NVIDIA 3080 (GPU 0 - Embeddings)
  NVIDIA 3070 (GPU 1 - Reasoning)
  NVIDIA P1000 (optional, display)
  NVMe storage recommended

Software Required
  docker --version
  docker compose version
  nvidia-smi

If Docker is missing:
  sudo apt install docker.io docker-compose-plugin

2. Enable NVIDIA GPU for Docker

Install NVIDIA Container Toolkit:
  distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
  curl -s -L https://nvidia.github.io/libnvidia-container/gpgkey | sudo apt-key add -
  curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
  sudo apt update
  sudo apt install -y nvidia-container-toolkit
  sudo systemctl restart docker

Verify:
  docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi

3. Getting Started

  cd ai_research
  docker compose up --build

First startup pulls Ollama models (nomic-embed-text, llama3) automatically.
This may take several minutes.

4. Architecture Overview (18 Containers)

Infrastructure (5 services):
  Service              Purpose                        Port
  PostgreSQL+pgvector  Database with vector search     localhost:5432
  Redis                Job queues & rate limiting      localhost:6379
  MinIO                Object/document storage         localhost:9000 (API), 9001 (console)
  Ollama (embed)       Embedding model server          localhost:11434
  Ollama (reason)      Reasoning model server          localhost:11435

Application Layer (1 service):
  Backend (FastAPI)    REST API                        localhost:8000

Pipeline Workers (6 services):
  worker-crawler       Fetches URLs, extracts text     jobs:crawl -> jobs:chunk
  worker-chunking      Splits documents into chunks    jobs:chunk -> jobs:embed
  worker-embedding     Generates vector embeddings     jobs:embed -> jobs:evaluate
  worker-evaluation    LLM scores trust/relevance      jobs:evaluate -> jobs:extract
  worker-extraction    Extracts DTCs, causes, steps    jobs:extract -> jobs:resolve
  worker-conflict      Resolves duplicate entities     jobs:resolve -> (terminal)

Monitoring & Self-Healing (2 services):
  monitor-agent        Metrics collection, alerting    localhost:8001
  healing-agent        Auto-fixes pipeline issues      (internal)

Autonomous Orchestration (4 services):
  orchestrator         OODA loop, task management      (internal)
  auditor              Quality/coverage analysis       (internal)
  researcher           URL discovery & submission      (internal)
  mcp-server           MCP knowledge API (SSE)         localhost:8002

5. Processing Pipeline

Documents flow through a 6-stage pipeline:

  POST /crawl or POST /ingest
      |
  Crawl -> Chunk -> Embed -> Evaluate -> Extract -> Resolve
      |         |        |          |          |         |
  Fetch URL  500-char  768-dim   Trust &   DTCs,     Merge
  or store   chunks    vectors   relevance causes,   duplicates
  in MinIO   w/overlap           scoring   steps

Each stage passes document IDs through Redis queues (FIFO via LPUSH/BRPOP).

6. Autonomous Operation

The system runs autonomously once started:

  Orchestrator (60s cycle):
    1. Observe - collect queue depths, task statuses, audit data
    2. Orient  - assess resource availability, coverage gaps
    3. Decide  - rule-based for routine, LLM for strategic planning
    4. Act     - dispatch research tasks, trigger audits, or wait

  Auditor (30-min cycle):
    - Confidence score distributions across all DTC codes
    - Completeness scoring per DTC (description, causes, steps, sensors, TSBs)
    - DTC range gap detection (e.g., P02xx has 5 codes, expected ~30)
    - Pipeline throughput, error rates, bottleneck detection
    - Stores reports in research.audit_reports

  Researcher (directive-driven):
    - Tier 1: Deterministic URL templates (obd-codes.com, engine-codes.com, etc.)
    - Tier 2: LLM-suggested URLs from known automotive resources
    - Rate limited: 30 URLs/hr total, 5/domain/hr, 30s cooldown
    - Validates URLs via HEAD request before submitting to crawl pipeline

  MCP Server (port 8002):
    - lookup_dtc: Full DTC detail with causes, steps, sensors, TSBs
    - search_knowledge: Semantic vector search across knowledge base
    - list_dtc_codes: List/filter codes by category and confidence
    - get_system_stats: Coverage and quality metrics

Inter-Agent Communication (Redis queues):
  orchestrator:research   orchestrator -> researcher
  orchestrator:audit      orchestrator -> auditor
  orchestrator:commands   auditor/researcher/API -> orchestrator

Resource Protection:
  - GPU available = evaluate + extract + embed queues < 20
  - Crawl slots available = crawl + chunk queues < 5
  - If pipeline busy, orchestrator defers new work
  - Existing pipeline workers always have priority

Environment Toggles:
  ORCHESTRATOR_AUTO_RESEARCH=true/false  Enable/disable autonomous research
  ORCHESTRATOR_CYCLE=60                  Seconds between orchestrator cycles
  AUDIT_INTERVAL=1800                    Seconds between automatic audits
  MAX_URLS_PER_HOUR=30                   Global URL submission rate limit
  MAX_PER_DOMAIN_PER_HOUR=5             Per-domain rate limit

7. API Reference

Health & Status:
  GET  /health                  System health (DB, Redis, MinIO connectivity)
  GET  /stats                   Document counts, queue depths, DTC stats

Document Management:
  POST /ingest                  Ingest raw text content
  POST /crawl                   Submit URL for crawling
  GET  /crawl                   List crawl queue entries
  GET  /documents               List documents (optional stage filter)
  GET  /documents/{id}          Document detail
  GET  /documents/{id}/chunks   Document chunks with evaluation scores
  GET  /documents/{id}/status   Processing status with full log

Search & Knowledge:
  POST /search                  Semantic vector search
  GET  /dtc                     List DTC codes (filter by category/confidence)
  GET  /dtc/{code}              Full DTC detail (causes, steps, sensors, TSBs)

Orchestration:
  GET  /orchestrator/status     Current orchestrator state and cycle info
  GET  /orchestrator/tasks      List orchestrator tasks (filter by status/type)
  POST /orchestrator/command    Submit manual command (trigger_audit, trigger_research)
  GET  /audit/latest            Latest audit report
  GET  /audit/reports           List audit reports
  GET  /coverage                Latest coverage snapshot
  GET  /research/plans          List research plans
  GET  /research/sources        List known source domains with quality tiers

8. Database Schema

research schema (Raw/Processing Layer):
  documents              Document metadata and processing state
  document_chunks        Chunked text with 768-dim vector embeddings
  chunk_evaluations      LLM trust/relevance scores per chunk
  crawl_queue            URL crawl job tracking
  processing_log         Audit log for all pipeline stages
  healing_log            Self-healing action audit trail
  orchestrator_tasks     Task management for orchestrator
  orchestrator_log       Orchestrator cycle audit trail
  research_sources       Domain quality and crawl tracking
  research_plans         Research plan tracking
  audit_reports          Quality audit reports
  coverage_snapshots     Daily DTC coverage snapshots

refined schema (Structured Knowledge Layer):
  dtc_codes              Diagnostic trouble codes with confidence scores
  dtc_sources            Links DTC codes to source chunks
  diagnostic_steps       Step-by-step diagnostic procedures
  causes                 Root causes per DTC code
  sensors                Automotive sensor catalog
  tsb_references         Technical Service Bulletins

9. GPU Configuration

Controlled via .env file:
  Single GPU (shared):   GPU_EMBED=all  GPU_REASON=all
  Dual GPU (isolated):   GPU_EMBED=0    GPU_REASON=1

Monitor GPU usage:
  watch -n1 nvidia-smi

10. Common Operations

Submit a URL for autonomous processing:
  curl -X POST http://localhost:8000/crawl \
    -H "Content-Type: application/json" \
    -d '{"url": "https://www.obd-codes.com/p0301"}'

Trigger a manual audit:
  curl -X POST http://localhost:8000/orchestrator/command \
    -H "Content-Type: application/json" \
    -d '{"action": "trigger_audit"}'

Trigger research for specific codes:
  curl -X POST http://localhost:8000/orchestrator/command \
    -H "Content-Type: application/json" \
    -d '{"action": "trigger_research", "target_codes": ["P0301", "P0302"]}'

Check system status:
  curl http://localhost:8000/stats
  curl http://localhost:8000/orchestrator/status
  curl http://localhost:8000/audit/latest
  curl http://localhost:8000/coverage

Search the knowledge base:
  curl -X POST http://localhost:8000/search \
    -H "Content-Type: application/json" \
    -d '{"query": "P0301 misfire causes", "limit": 5}'

Look up a DTC code:
  curl http://localhost:8000/dtc/P0301

Connect MCP client to:
  http://localhost:8002/sse

11. Backups

Backup Postgres:
  docker exec refinery_postgres pg_dump -U refinery refinery > backup.sql

Restore:
  cat backup.sql | docker exec -i refinery_postgres psql -U refinery refinery

MinIO data is in the minio_data Docker volume.

12. Troubleshooting

View logs for any service:
  docker compose logs -f orchestrator
  docker compose logs -f auditor
  docker compose logs -f researcher
  docker compose logs -f mcp-server
  docker compose logs -f worker-crawler

Check container status:
  docker compose ps

Restart a specific service:
  docker compose restart orchestrator

GPU not detected in Ollama:
  docker exec -it refinery_llm_embed nvidia-smi
  If empty -> NVIDIA container toolkit not configured (see section 2)

Database connection issues:
  docker compose logs postgres
  Ensure postgres container is healthy: docker compose ps

Reset database (destructive):
  docker compose down -v
  docker compose up --build
