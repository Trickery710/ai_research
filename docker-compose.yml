services:
  # ----------------------------------------------------------
  # INFRASTRUCTURE
  # ----------------------------------------------------------
  postgres:
    image: ankane/pgvector:latest
    container_name: refinery_postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-refinery}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-refinery}
      POSTGRES_DB: refinery
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./db/init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "127.0.0.1:5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-refinery} -d refinery"]
      interval: 5s
      timeout: 5s
      retries: 10
    networks:
      - refinery

  redis:
    image: redis:7
    container_name: refinery_redis
    restart: unless-stopped
    command: >
      sh -c "if [ -f /etc/redis/redis.conf ]; then
        redis-server /etc/redis/redis.conf;
      else
        redis-server;
      fi"
    volumes:
      - ./secrets/redis.conf:/etc/redis/redis.conf:ro
    ports:
      - "127.0.0.1:6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD:-}", "ping"]
      interval: 5s
      timeout: 3s
      retries: 10
    networks:
      - refinery

  minio:
    image: minio/minio
    container_name: refinery_minio
    restart: unless-stopped
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minioadmin}
    ports:
      - "127.0.0.1:9000:9000"
      - "127.0.0.1:9001:9001"
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - refinery

  # ----------------------------------------------------------
  # LLM SERVICES (GPU-accelerated Ollama instances)
  # GPU assignment is controlled via .env file:
  #   Single GPU (default):  GPU_EMBED=all, GPU_REASON=all (shared)
  #   Dual GPU (production): GPU_EMBED=0,   GPU_REASON=1   (isolated)
  # ----------------------------------------------------------
  llm-embed:
    image: ollama/ollama
    container_name: refinery_llm_embed
    restart: unless-stopped
    ports:
      - "127.0.0.1:11434:11434"
    volumes:
      - ollama_embed_data:/root/.ollama
    environment:
      OLLAMA_HOST: "0.0.0.0:11434"
      NVIDIA_VISIBLE_DEVICES: ${GPU_EMBED:-all}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 30s
    networks:
      - refinery

  llm-reason:
    image: ollama/ollama
    container_name: refinery_llm_reason
    restart: unless-stopped
    ports:
      - "127.0.0.1:11435:11434"
    volumes:
      - ollama_reason_data:/root/.ollama
    environment:
      OLLAMA_HOST: "0.0.0.0:11434"
      NVIDIA_VISIBLE_DEVICES: ${GPU_REASON:-all}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 30s
    networks:
      - refinery

  # Second reasoning instance on GPU 0 (alongside embeddings)
  # The 3080 has 10GB - embeddings use ~274MB, leaving ~9.7GB for reasoning
  llm-reason2:
    image: ollama/ollama
    container_name: refinery_llm_reason2
    restart: unless-stopped
    volumes:
      - ollama_reason2_data:/root/.ollama
    environment:
      OLLAMA_HOST: "0.0.0.0:11434"
      NVIDIA_VISIBLE_DEVICES: ${GPU_EMBED:-all}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 30s
    networks:
      - refinery

  # ----------------------------------------------------------
  # BACKEND API
  # ----------------------------------------------------------
  backend:
    build: ./backend
    container_name: refinery_backend
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      minio:
        condition: service_healthy
    environment:
      DATABASE_URL: ${DATABASE_URL:-postgresql://refinery:refinery@postgres:5432/refinery}
      REDIS_HOST: redis
      REDIS_PORT: "6379"
      REDIS_PASSWORD: ${REDIS_PASSWORD:-}
      MINIO_ENDPOINT: minio:9000
      MINIO_ACCESS_KEY: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_SECRET_KEY: ${MINIO_ROOT_PASSWORD:-minioadmin}
      MINIO_BUCKET: documents
      OLLAMA_EMBED_URL: "http://llm-embed:11434"
      EMBEDDING_MODEL: nomic-embed-text
      API_KEYS: ${API_KEYS:-}
    ports:
      - "8000:8000"
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import urllib.request; urllib.request.urlopen('http://localhost:8000/health')\""]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - refinery

  # ----------------------------------------------------------
  # WORKERS
  # ----------------------------------------------------------
  worker-chunking:
    build:
      context: ./workers
      dockerfile: chunking/Dockerfile
    container_name: refinery_worker_chunking
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      minio:
        condition: service_healthy
    environment:
      DATABASE_URL: ${DATABASE_URL:-postgresql://refinery:refinery@postgres:5432/refinery}
      REDIS_HOST: redis
      REDIS_PORT: "6379"
      REDIS_PASSWORD: ${REDIS_PASSWORD:-}
      MINIO_ENDPOINT: minio:9000
      MINIO_ACCESS_KEY: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_SECRET_KEY: ${MINIO_ROOT_PASSWORD:-minioadmin}
      MINIO_BUCKET: documents
      WORKER_QUEUE: "jobs:chunk"
      NEXT_QUEUE: "jobs:embed"
    networks:
      - refinery

  worker-embedding:
    build:
      context: ./workers
      dockerfile: embedding/Dockerfile
    container_name: refinery_worker_embedding
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      llm-embed:
        condition: service_healthy
    environment:
      DATABASE_URL: ${DATABASE_URL:-postgresql://refinery:refinery@postgres:5432/refinery}
      REDIS_HOST: redis
      REDIS_PORT: "6379"
      REDIS_PASSWORD: ${REDIS_PASSWORD:-}
      OLLAMA_BASE_URL: "http://llm-embed:11434"
      EMBEDDING_MODEL: nomic-embed-text
      WORKER_QUEUE: "jobs:embed"
      NEXT_QUEUE: "jobs:evaluate"
    networks:
      - refinery

  worker-evaluation:
    build:
      context: ./workers
      dockerfile: evaluation/Dockerfile
    container_name: refinery_worker_evaluation
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      llm-reason:
        condition: service_healthy
    environment:
      DATABASE_URL: ${DATABASE_URL:-postgresql://refinery:refinery@postgres:5432/refinery}
      REDIS_HOST: redis
      REDIS_PORT: "6379"
      REDIS_PASSWORD: ${REDIS_PASSWORD:-}
      OLLAMA_BASE_URL: "http://llm-reason:11434"
      REASONING_MODEL: llama3.1
      WORKER_QUEUE: "jobs:evaluate"
      NEXT_QUEUE: "jobs:extract"
    networks:
      - refinery

  worker-extraction:
    build:
      context: ./workers
      dockerfile: extraction/Dockerfile
    container_name: refinery_worker_extraction
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      llm-reason:
        condition: service_healthy
    environment:
      DATABASE_URL: ${DATABASE_URL:-postgresql://refinery:refinery@postgres:5432/refinery}
      REDIS_HOST: redis
      REDIS_PORT: "6379"
      REDIS_PASSWORD: ${REDIS_PASSWORD:-}
      OLLAMA_BASE_URL: "http://llm-reason:11434"
      REASONING_MODEL: llama3.1.1
      WORKER_QUEUE: "jobs:extract"
      NEXT_QUEUE: "jobs:resolve"
    networks:
      - refinery

  worker-conflict:
    build:
      context: ./workers
      dockerfile: conflict/Dockerfile
    container_name: refinery_worker_conflict
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      DATABASE_URL: ${DATABASE_URL:-postgresql://refinery:refinery@postgres:5432/refinery}
      REDIS_HOST: redis
      REDIS_PORT: "6379"
      REDIS_PASSWORD: ${REDIS_PASSWORD:-}
      WORKER_QUEUE: "jobs:resolve"
      NEXT_QUEUE: ""
    networks:
      - refinery

  worker-crawler:
    build:
      context: ./workers
      dockerfile: crawler/Dockerfile
    container_name: refinery_worker_crawler
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      minio:
        condition: service_healthy
    environment:
      DATABASE_URL: ${DATABASE_URL:-postgresql://refinery:refinery@postgres:5432/refinery}
      REDIS_HOST: redis
      REDIS_PORT: "6379"
      REDIS_PASSWORD: ${REDIS_PASSWORD:-}
      MINIO_ENDPOINT: minio:9000
      MINIO_ACCESS_KEY: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_SECRET_KEY: ${MINIO_ROOT_PASSWORD:-minioadmin}
      MINIO_BUCKET: documents
      WORKER_QUEUE: "jobs:crawl"
      NEXT_QUEUE: "jobs:chunk"
    networks:
      - refinery

  # ----------------------------------------------------------
  # MONITORING & SELF-HEALING
  # ----------------------------------------------------------
  monitor-agent:
    build:
      context: ./workers
      dockerfile: monitoring/Dockerfile
    container_name: refinery_monitor_agent
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      backend:
        condition: service_healthy
    environment:
      DATABASE_URL: ${DATABASE_URL:-postgresql://refinery:refinery@postgres:5432/refinery}
      REDIS_HOST: redis
      REDIS_PORT: "6379"
      REDIS_PASSWORD: ${REDIS_PASSWORD:-}
      BACKEND_URL: "http://backend:8000"
      MONITOR_INTERVAL: "45"
      QUEUE_STALL_THRESHOLD: "300"
      ERROR_RATE_THRESHOLD: "0.15"
      ALERT_QUEUE: "monitoring:alerts"
    ports:
      - "127.0.0.1:8001:8001"
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import urllib.request; urllib.request.urlopen('http://localhost:8001/health')\""]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 20s
    networks:
      - refinery

  healing-agent:
    build:
      context: ./workers
      dockerfile: healing/Dockerfile
    container_name: refinery_healing_agent
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      llm-reason:
        condition: service_healthy
      monitor-agent:
        condition: service_healthy
    environment:
      DATABASE_URL: ${DATABASE_URL:-postgresql://refinery:refinery@postgres:5432/refinery}
      REDIS_HOST: redis
      REDIS_PORT: "6379"
      REDIS_PASSWORD: ${REDIS_PASSWORD:-}
      OLLAMA_BASE_URL: "http://llm-reason:11434"
      REASONING_MODEL: llama3.1
      AUTO_FIX_ENABLED: "true"
      AUTO_FIX_ALLOW: "restart_worker,requeue_documents,clear_stale_locks"
      AUTO_FIX_DENY: "restart_container,database_operations,delete_data"
      MAX_ACTIONS_PER_HOUR: "10"
      COOLDOWN_BETWEEN_ACTIONS: "120"
      ALERT_QUEUE: "monitoring:alerts"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - refinery

  # ----------------------------------------------------------
  # AUTONOMOUS ORCHESTRATION
  # ----------------------------------------------------------
  auditor:
    build:
      context: ./workers
      dockerfile: auditor/Dockerfile
    container_name: refinery_auditor
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      PYTHONUNBUFFERED: "1"
      DATABASE_URL: ${DATABASE_URL:-postgresql://refinery:refinery@postgres:5432/refinery}
      REDIS_HOST: redis
      REDIS_PORT: "6379"
      REDIS_PASSWORD: ${REDIS_PASSWORD:-}
      AUDIT_INTERVAL: ${AUDIT_INTERVAL:-1800}
    networks:
      - refinery

  orchestrator:
    build:
      context: ./workers
      dockerfile: orchestrator/Dockerfile
    container_name: refinery_orchestrator
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      PYTHONUNBUFFERED: "1"
      DATABASE_URL: ${DATABASE_URL:-postgresql://refinery:refinery@postgres:5432/refinery}
      REDIS_HOST: redis
      REDIS_PORT: "6379"
      REDIS_PASSWORD: ${REDIS_PASSWORD:-}
      OLLAMA_BASE_URL: "http://llm-reason:11434"
      REASONING_MODEL: llama3.1
      ORCHESTRATOR_CYCLE: ${ORCHESTRATOR_CYCLE:-60}
      ORCHESTRATOR_AUTO_RESEARCH: ${ORCHESTRATOR_AUTO_RESEARCH:-true}
      MAX_GPU_QUEUE_ITEMS: "20"
      MAX_CONCURRENT_CRAWLS: "5"
    networks:
      - refinery

  researcher:
    build:
      context: ./workers
      dockerfile: researcher/Dockerfile
    container_name: refinery_researcher
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      PYTHONUNBUFFERED: "1"
      DATABASE_URL: ${DATABASE_URL:-postgresql://refinery:refinery@postgres:5432/refinery}
      REDIS_HOST: redis
      REDIS_PORT: "6379"
      REDIS_PASSWORD: ${REDIS_PASSWORD:-}
      OLLAMA_BASE_URL: "http://llm-reason:11434"
      REASONING_MODEL: llama3.1
      MAX_URLS_PER_HOUR: ${MAX_URLS_PER_HOUR:-30}
      MAX_PER_DOMAIN_PER_HOUR: ${MAX_PER_DOMAIN_PER_HOUR:-5}
      RESEARCH_COOLDOWN: "30"
    networks:
      - refinery

  mcp-server:
    build:
      context: ./workers
      dockerfile: mcp-server/Dockerfile
    container_name: refinery_mcp_server
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      llm-embed:
        condition: service_healthy
    environment:
      PYTHONUNBUFFERED: "1"
      DATABASE_URL: ${DATABASE_URL:-postgresql://refinery:refinery@postgres:5432/refinery}
      REDIS_HOST: redis
      REDIS_PORT: "6379"
      REDIS_PASSWORD: ${REDIS_PASSWORD:-}
      OLLAMA_BASE_URL: "http://llm-embed:11434"
      EMBEDDING_MODEL: nomic-embed-text
    ports:
      - "8002:8002"
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import urllib.request; urllib.request.urlopen('http://localhost:8002/health')\""]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 20s
    networks:
      - refinery

networks:
  refinery:
    driver: bridge

volumes:
  postgres_data:
  minio_data:
  ollama_embed_data:
  ollama_reason_data:
  ollama_reason2_data:
